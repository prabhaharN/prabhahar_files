import pandas as pd


orders= {
    "order_id": [1, 2, 3, 4, 5],
    "user_id": [101, 102, 101, 103, 102],
    "order_date": ["2023-05-01", "2023-05-03", "2023-05-07", "2023-05-10", "2023-05-12"],
    "amount": [100.0, 50.0, 150.0, 200.0, 80.0]
}

df_orders = pd.DataFrame(orders)


users= {
    "user_id": [101, 102, 103],
    "join_date": ["2023-01-01", "2023-02-15", "2023-04-05"],
    "location": ["Seattle", "Portland", "San Jose"]
}

df_users = pd.DataFrame(users)

1)Find the total amount spent by each user on their orders.

df_orders.groupby('user_id')['amount'].sum()

2)Identify users who have placed more than oneorder. Provide their user_id, location, and totalnumber of orders placed.

mer_df = pd.merge(df_orders,df_users, on = 'user_id',how = 'left')
ord_c = mer_df.groupby(['user_id','location']).size().reset_index(name='order_count')
ord_c[ord_c['order_count']>1]



3)For each location, find the average order amount and the latest order date.
mer_df['order_date'] = pd.to_datetime(mer_df['order_date'])
location_stats = mer_df.groupby('location').agg(vg_order_amount=('amount', 'mean'),latest_order_date=('order_date', 'max')).reset_index()
location_stats

4)Find the user_id of individuals who have made an order amounting to more than 100 but haven't made any orders after May 7, 2023.

filtered_df = mer_df[(mer_df['amount'] > 100) & (mer_df['order_date'] <= '2023-05-07')]
filtered_df
--------------------------------------------------------------------------

posts = {
    "post_id": [1, 2, 3, 4, 5],
    "user_id": [201, 202, 201, 203, 202],
    "post_date": ["2023-03-15", "2023-03-16", "2023-03-17", "2023-03-18", "2023-03-19"],
    "content": ["Photo", "Text", "Video", "Photo", "Link"]
}

df_posts = pd.DataFrame(posts)

reactions = {
    "react_id": [1, 2, 3, 4, 5],
    "post_id": [1, 2, 3, 4, 1],
    "user_id": [202, 203, 201, 201, 203],
    "reaction": ["Like", "Love", "Wow", "Haha", "Angry"]
}



df_reactions = pd.DataFrame(reactions)





1)Find the posts that have received reactions from their own authors. List the post_id and reaction.

mer_df = pd.merge(df_posts,df_reactions,on='post_id',how = 'inner')
mer_df

sel_tag =mer_df[mer_df['user_id_x'] == mer_df['user_id_y']][['post_id', 'reaction']]
sel_tag

2)For each type of reaction (Like, Love, etc.), calculate the total number of occurrences.

mer_df.groupby('reaction').size().reset_index(name = 'count')

3)Find users who have posted content but have never reacted to any post, including their own.

user_pos = df_posts['user_id'].unique()
user_rec = df_reactions['user_id'].unique()

user_no_rec = set(user_pos) - set(user_rec)
print(user_no_rec)

 4)For each user, identify the total number of reactions they've received on their posts. If they haven't received any reactions, they should still appear in the result with a count of 0.

lef_mer = pd.merge(df_posts,df_reactions,on='post_id',how = 'left')
reaction_counts = lef_mer.groupby('user_id_x')['react_id'].count().reset_index()
reaction_counts


reaction_counts.columns = ['user_id', 'reaction_count']

all_users = pd.DataFrame(df_posts['user_id'].unique(), columns=['user_id'])
result = pd.merge(all_users, reaction_counts, on='user_id', how='left').fillna(0)

result

--------------------------------------------------------------------------------------------------------------------------------------

trips_ = {
    "trip_id": [1, 2, 3, 4, 5],
    "vehicle_id": [501, 502, 501, 503, 502],
    "start_time": ["2023-05-01", "2023-05-02", "2023-05-03", "2023-05-04", "2023-05-05"],
    "end_time": ["2023-05-01", "2023-05-02", "2023-05-03", "2023-05-04", "2023-05-05"]
}
df_trips = pd.DataFrame(trips)

sensors = {
    "sensor_id": [1, 2, 3, 4, 5],
    "trip_id": [1, 2, 3, 4, 2],
    "sensor_type": ["Lidar", "Camera", "GPS", "Lidar", "GPS"],
    "status": ["Good", "Fault", "Good", "Good", "Good"],
    "readings": [3521, 0, 9872, 3612, 9654]
}

df_sensors = pd.DataFrame(sensors)

1)Find all trips where at least one sensor reported a fault. List the trip_id and the corresponding sensor_type.

mer_df = pd.merge(df_sensors,df_trips,on= 'trip_id',how = 'inner')
mer_df
mer_df[mer_df['status']=='Fault'][['trip_id', 'sensor_type']]

 2)For each vehicle, calculate the average readings for each sensor type when the status is "Good". List the vehicle_id,sensor_type, and average readings.

good_stat = mer_df[mer_df['status']=='Good']
good_stat
res = good_stat.groupby(['vehicle_id', 'sensor_type'])['readings'].mean().reset_index()
res

3)Identify the vehicle with the highest number of trips. Provide the vehicle_id and the total number of trips.

trip = df_trips.groupby('vehicle_id')['trip_id'].count().reset_index(name = 'total_trips')
vehicle_with_max_trips  = trip.loc[trip['total_trips']].max()
vehicle_with_max_trips

4)Which sensor type has the most recorded data across all trips (based on readings)? Provide the sensor_type and the accumulated readings.

tota_read = mer_df.groupby('sensor_type')['readings'].sum().reset_index()
sensor_with_max_readings = tota_read.loc[tota_read['readings'].idxmax()]
sensor_with_max_readings
--------------------------------------------------------------------------------------------------------------------------------------

appstore_data = {
    "app_id": [1, 2, 3, 4, 5],
    "app_title": ["iChat", "iPhotoMagic", "iTuneUp", "iFixTools", "iRead"],
    "genre": ["Social", "Photo", "Music", "Utility", "Books"],
    "price": [0.00, 2.99, 0.00, 4.99, 0.00]
}
purchases_data = {
    "purchase_id": [1, 2, 3, 4, 5],
    "app_id": [1, 2, 1, 3, 2],
    "user_id": [401, 402, 403, 401, 401],
    "purchase_date": ["2023-04-10", "2023-04-11", "2023-04-12", "2023-04-13", "2023-04-14"]
}

df_appstore = pd.DataFrame(appstore_data)
df_purchases = pd.DataFrame(purchases_data)


1)Identify the most expensive app in the App Store. Provide the app_title, genre, and price.
expen = df_appstore.sort_values(by = 'price',ascending=False).iloc[0]
expen.reset_index()

#2)Calculate the total earnings from the Utilitygenre.
uti = df_appstore[df_appstore['genre'] =='Utility']
tot_earn = uti['price'].max()
tot_earn

3)Which users have spent the most money on apps? Provide the user_id and the total amount spent.

mer_df = pd.merge(df_appstore,df_purchases,on='app_id',how = 'inner')
users = mer_df.groupby('user_id')['price'].sum()
users

4)Determine the genre popularity based on purchases.Provide the genre and its corresponding number of purchases, sorted in descending order of popularity.
genre_popularity = mer_df.groupby('genre')['purchase_id'].count().reset_index()
genre_popularity = genre_popularity.rename(columns={'purchase_id': 'number_of_purchases'})
genre_popularity_sorted = genre_popularity.sort_values(by='number_of_purchases', ascending=False)
genre_popularity_sorted
--------------------------------------------------------------------------------------------------------------------------------------


shows = {
    "show_id": [1, 2, 3, 4, 5],
    "title": ["Streamer's Life", "The Lost Byte", "Data Love", "Query's End", "Join Junction"],
    "genre": ["Comedy", "Thriller", "Romance", "Sci-Fi", "Drama"],
    "seasons": [3, 1, 2, 4, 2]
}
df_shows = pd.DataFrame(shows)

views= {
    "view_id": [1, 2, 3, 4, 5],
    "show_id": [1, 2, 1, 3, 2],
    "user_id": [501, 502, 503, 501, 501],
    "view_date": ["2023-05-01", "2023-05-02", "2023-05-03", "2023-05-04", "2023-05-05"],
    "season": [1, 1, 2, 1, 1]
}


df_views = pd.DataFrame(views)


1)Which show has the highest number of unique viewers? Provide the title and the count of viewers.
unique_viewers_per_show = df_views.groupby('show_id')['user_id'].nunique().reset_index()
unique_viewers_per_show = unique_viewers_per_show.rename(columns={'user_id': 'unique_viewers'})

max_unique_viewers_show = unique_viewers_per_show.loc[unique_viewers_per_show['unique_viewers'].idxmax()]
max_unique_viewers_show_info = df_shows[df_shows['show_id'] == max_unique_viewers_show['show_id']]
max_unique_viewers_show_title = max_unique_viewers_show_info.iloc[0]['title']
max_unique_viewers_count = max_unique_viewers_show['unique_viewers']
print(f"the show with the highest number of unique viewers is '{max_unique_viewers_show_title}' with {max_unique_viewers_count} views.")

2)Calculate the total views per genre.
merged_df = pd.merge(df_shows, df_views, on='show_id', how='inner')

views_per_genre = merged_df.groupby('genre')['view_id'].count().reset_index()
views_per_genre = views_per_genre.rename(columns={'view_id': 'total_views'})
views_per_genre

3)Identify users who have watched all the seasons of aparticular show.

show_id_to_check = 1
total_seasons_to_watch = df_shows[df_shows['show_id'] == show_id_to_check]['seasons'].iloc[0]
views_for_show = df_views[df_views['show_id'] == show_id_to_check]
users_watched_all_seasons  = views_for_show.groupby('user_id')['season'].nunique() == total_seasons_to_watch
user_ids_watched_all_seasons  = users_watched_all_seasons[users_watched_all_seasons].index.tolist()
user_ids_watched_all_seasons

4)Find out which day had the highest streamingactivity. Provide the view_date and the number ofviews for that day.
views_per_day = df_views.groupby('view_date')['view_id'].count().reset_index()
views_per_day = views_per_day.rename(columns={'view_id': 'total_views'})
day_highest_activity = views_per_day.loc[views_per_day['total_views'].idxmax()]
day_highest_activity